{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading .env from: c:\\Users\\savan\\Desktop\\portfolio_repo\\arb_airdrop\\.env\n"
     ]
    }
   ],
   "source": [
    "#API connection and dependency install\n",
    "import os\n",
    "from flipside import Flipside\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Confirm and define dotenv path\n",
    "dotenv_path = find_dotenv()\n",
    "print(f\"Loading .env from: {dotenv_path}\")\n",
    "\n",
    "#load env data from path\n",
    "load_dotenv(dotenv_path, override=True)\n",
    "\n",
    "#store API_KEY from dotenv file\n",
    "import os\n",
    "API_KEY = os.getenv(\"FLIP_API_KEY\")\n",
    "# print(f\"API Key loaded: {API_KEY}\")\n",
    "\n",
    "#instantiate flipside client\n",
    "flipside = Flipside(API_KEY, \"https://api-v2.flipsidecrypto.xyz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs('data/processed/transfers_batches', exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load airdrop recipient wallets as a list. Query wallet transfers in batches from ez_token_transfers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 0 to 1000 with 12159 rows\n",
      "Saved batch 1000 to 2000 with 100000 rows\n",
      "Saved batch 2000 to 3000 with 10183 rows\n",
      "Saved batch 3000 to 4000 with 12103 rows\n",
      "Saved batch 4000 to 5000 with 22875 rows\n",
      "Saved batch 5000 to 6000 with 10072 rows\n",
      "Saved batch 6000 to 7000 with 15367 rows\n",
      "Saved batch 7000 to 8000 with 9428 rows\n",
      "Saved batch 8000 to 9000 with 8871 rows\n",
      "Saved batch 9000 to 10000 with 9360 rows\n",
      "Saved batch 10000 to 11000 with 8172 rows\n",
      "Saved batch 11000 to 12000 with 7735 rows\n",
      "Saved batch 12000 to 13000 with 18951 rows\n",
      "Saved batch 13000 to 14000 with 8089 rows\n",
      "Saved batch 14000 to 15000 with 14336 rows\n",
      "Saved batch 15000 to 16000 with 8362 rows\n",
      "Saved batch 16000 to 17000 with 7140 rows\n",
      "Saved batch 17000 to 18000 with 8973 rows\n",
      "Saved batch 18000 to 19000 with 13986 rows\n",
      "Saved batch 19000 to 20000 with 11196 rows\n",
      "Saved batch 20000 to 21000 with 11072 rows\n",
      "Saved batch 21000 to 22000 with 11750 rows\n",
      "Saved batch 22000 to 23000 with 10164 rows\n",
      "Saved batch 23000 to 24000 with 13286 rows\n",
      "Saved batch 24000 to 25000 with 9898 rows\n",
      "Saved batch 25000 to 26000 with 11991 rows\n",
      "Saved batch 26000 to 27000 with 12722 rows\n",
      "Saved batch 27000 to 28000 with 11637 rows\n",
      "Saved batch 28000 to 29000 with 15184 rows\n",
      "Saved batch 29000 to 30000 with 24288 rows\n",
      "Saved batch 30000 to 31000 with 11543 rows\n",
      "Saved batch 31000 to 32000 with 9717 rows\n",
      "Saved batch 32000 to 33000 with 10680 rows\n",
      "Saved batch 33000 to 34000 with 8917 rows\n",
      "Saved batch 34000 to 35000 with 10601 rows\n",
      "Saved batch 35000 to 36000 with 11869 rows\n",
      "Saved batch 36000 to 37000 with 13309 rows\n",
      "Saved batch 37000 to 38000 with 10548 rows\n",
      "Saved batch 38000 to 39000 with 9470 rows\n",
      "Saved batch 39000 to 40000 with 11028 rows\n",
      "Saved batch 40000 to 41000 with 10877 rows\n",
      "Saved batch 41000 to 42000 with 13127 rows\n",
      "Saved batch 42000 to 43000 with 11379 rows\n",
      "Saved batch 43000 to 44000 with 6494 rows\n",
      "Saved batch 44000 to 45000 with 6978 rows\n",
      "Saved batch 45000 to 46000 with 11201 rows\n",
      "Saved batch 46000 to 47000 with 14880 rows\n",
      "Saved batch 47000 to 48000 with 12550 rows\n",
      "Saved batch 48000 to 49000 with 9633 rows\n",
      "Saved batch 49000 to 50000 with 11840 rows\n",
      "Saved batch 50000 to 51000 with 9602 rows\n",
      "Saved batch 51000 to 52000 with 11533 rows\n",
      "Saved batch 52000 to 53000 with 12564 rows\n",
      "Saved batch 53000 to 54000 with 10857 rows\n",
      "Saved batch 54000 to 55000 with 10920 rows\n",
      "Saved batch 55000 to 56000 with 15454 rows\n",
      "Saved batch 56000 to 57000 with 13782 rows\n",
      "Saved batch 57000 to 58000 with 13391 rows\n",
      "Saved batch 58000 to 59000 with 12839 rows\n",
      "Saved batch 59000 to 60000 with 9694 rows\n",
      "Saved batch 60000 to 61000 with 11318 rows\n",
      "Saved batch 61000 to 62000 with 9764 rows\n",
      "Saved batch 62000 to 63000 with 8133 rows\n",
      "Saved batch 63000 to 64000 with 10900 rows\n",
      "Saved batch 64000 to 65000 with 9883 rows\n",
      "Saved batch 65000 to 66000 with 11250 rows\n",
      "Saved batch 66000 to 67000 with 9294 rows\n",
      "Saved batch 67000 to 68000 with 13133 rows\n",
      "Saved batch 68000 to 69000 with 11773 rows\n",
      "Saved batch 69000 to 70000 with 10059 rows\n",
      "Saved batch 70000 to 71000 with 9948 rows\n",
      "Saved batch 71000 to 72000 with 10905 rows\n",
      "Saved batch 72000 to 73000 with 11536 rows\n",
      "Saved batch 73000 to 74000 with 9713 rows\n",
      "Saved batch 74000 to 75000 with 12387 rows\n",
      "Saved batch 75000 to 76000 with 7574 rows\n",
      "Saved batch 76000 to 77000 with 9812 rows\n",
      "Saved batch 77000 to 78000 with 9636 rows\n",
      "Saved batch 78000 to 79000 with 10268 rows\n",
      "Saved batch 79000 to 80000 with 12622 rows\n",
      "Saved batch 80000 to 81000 with 10953 rows\n",
      "Saved batch 81000 to 82000 with 11245 rows\n",
      "Saved batch 82000 to 83000 with 9149 rows\n",
      "Saved batch 83000 to 84000 with 10229 rows\n",
      "Saved batch 84000 to 85000 with 9514 rows\n",
      "Saved batch 85000 to 86000 with 16651 rows\n",
      "Saved batch 86000 to 87000 with 13768 rows\n",
      "Saved batch 87000 to 88000 with 8789 rows\n",
      "Saved batch 88000 to 89000 with 9967 rows\n",
      "Saved batch 89000 to 90000 with 9844 rows\n",
      "Saved batch 90000 to 91000 with 12010 rows\n",
      "Saved batch 91000 to 92000 with 13151 rows\n",
      "Saved batch 92000 to 93000 with 10779 rows\n",
      "Saved batch 93000 to 94000 with 10451 rows\n",
      "Saved batch 94000 to 95000 with 11192 rows\n",
      "Saved batch 95000 to 96000 with 8956 rows\n",
      "Saved batch 96000 to 97000 with 10391 rows\n",
      "Saved batch 97000 to 98000 with 10036 rows\n",
      "Saved batch 98000 to 99000 with 10367 rows\n",
      "Saved batch 99000 to 100000 with 9690 rows\n"
     ]
    }
   ],
   "source": [
    "recipients_df = pd.read_csv(r\"C:\\Users\\savan\\Desktop\\portfolio_repo\\arb_airdrop\\data\\processed\\arb_drop_data\")\n",
    "\n",
    "batch_size = 1000\n",
    "wallet_list = recipients_df['user_address'].unique()\n",
    "\n",
    "for i in range(0, len(wallet_list), batch_size):\n",
    "    batch = wallet_list[i:i+batch_size].tolist()\n",
    "    batch_str = \"','\".join(batch)\n",
    "    \n",
    "    sql = f\"\"\"\n",
    "        SELECT block_number, block_timestamp, contract_address, from_address, to_address, amount\n",
    "        FROM arbitrum.core.ez_token_transfers\n",
    "        WHERE (from_address IN ('{batch_str}') OR to_address IN ('{batch_str}'))\n",
    "        AND contract_address = LOWER('0x912CE59144191C1204E64559FE8253a0e49E6548')\n",
    "    \"\"\"\n",
    "    \n",
    "    wallet_transfers = flipside.query(sql)\n",
    "    \n",
    "    columns = wallet_transfers.columns\n",
    "    rows = wallet_transfers.rows  \n",
    "    wallet_transfers_df = pd.DataFrame(rows, columns=columns)\n",
    "    wallet_transfers_df.to_parquet(f\"data/processed/transfers_batches/transfers_batch_{i}.parquet\", index=False)\n",
    "    print(f\"Saved batch {i} to {i+batch_size} with {len(wallet_transfers_df)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stitch together files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m batch_files = glob.glob(\u001b[33m\"\u001b[39m\u001b[33mnotebooks/data/transfers_batches/transfers_batch_*.parquet\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m transfers_dfs = [pd.read_parquet(f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m batch_files]\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m transfers_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransfers_dfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCombined all transfer batches: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(transfers_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rows\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\savan\\Desktop\\portfolio_repo\\arb_airdrop\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:382\u001b[39m, in \u001b[36mconcat\u001b[39m\u001b[34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[39m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[32m    380\u001b[39m     copy = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m382\u001b[39m op = \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m op.get_result()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\savan\\Desktop\\portfolio_repo\\arb_airdrop\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:445\u001b[39m, in \u001b[36m_Concatenator.__init__\u001b[39m\u001b[34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[39m\n\u001b[32m    442\u001b[39m \u001b[38;5;28mself\u001b[39m.verify_integrity = verify_integrity\n\u001b[32m    443\u001b[39m \u001b[38;5;28mself\u001b[39m.copy = copy\n\u001b[32m--> \u001b[39m\u001b[32m445\u001b[39m objs, keys = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_clean_keys_and_objs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[32m    448\u001b[39m ndims = \u001b[38;5;28mself\u001b[39m._get_ndims(objs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\savan\\Desktop\\portfolio_repo\\arb_airdrop\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:507\u001b[39m, in \u001b[36m_Concatenator._clean_keys_and_objs\u001b[39m\u001b[34m(self, objs, keys)\u001b[39m\n\u001b[32m    504\u001b[39m     objs_list = \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[32m    506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m507\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo objects to concatenate\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    509\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    510\u001b[39m     objs_list = \u001b[38;5;28mlist\u001b[39m(com.not_none(*objs_list))\n",
      "\u001b[31mValueError\u001b[39m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "batch_files = glob.glob(\"notebooks/data/transfers_batches/transfers_batch_*.parquet\")\n",
    "transfers_dfs = [pd.read_parquet(f) for f in batch_files]\n",
    "transfers_df = pd.concat(transfers_dfs, ignore_index=True)\n",
    "print(f\"Combined all transfer batches: {len(transfers_df)} rows\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
